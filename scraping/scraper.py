import requests
from bs4 import BeautifulSoup
import dateparser
import datetime
import time
import schedule

# ‚úÖ List of diseases to search for
DISEASES = [
    "Dengue", "Nipah", "COVID-19", "Swine Flu", "Cholera", "Malaria",
    "Hepatitis", "Typhoid", "Leptospirosis", "Chikungunya", "Jaundice",
    "Conjunctivitis", "Food Poisoning", "H1N1", "Tuberculosis",
    "Pneumonia", "Skin Infection", "Asthma", "Fungal Infection",
    "Diarrhea", "Common Cold", "Cough", "Fever", "Stomach Infection",
    "Viral Fever", "Dysentery", "Allergy", "Sinusitis"
]

# ‚úÖ Cities in Kerala to track
KERALA_CITIES = [
    "Thiruvananthapuram", "Kochi", "Kozhikode", "Thrissur", "Kottayam",
    "Palakkad", "Alappuzha", "Pathanamthitta", "Malappuram", "Kasaragod",
    "Kannur", "Idukki", "Ernakulam", "Wayanad", "Kollam"
]

# ‚úÖ File to store the scraped data (TEXT file)
OUTPUT_FILE = "kerala_disease_data.txt"

# ‚úÖ Function to scrape news for a specific disease
def scrape_news(disease):
    print(f"üîç Fetching latest news for: {disease}...")

    query = f"{disease} outbreak in Kerala"
    url = f"https://www.google.com/search?q={query}&tbm=nws"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    news_data = []

    for article in soup.find_all("div", class_="SoaBEf"):
        title = article.find("div", class_="n0jPhd").get_text(strip=True) if article.find("div", class_="n0jPhd") else "Unknown"
        location = "Kerala"  # Default location if not found
        severity = "Low"  # Default severity

        # ‚úÖ Extract news timestamp from Google News metadata
        time_element = article.find_next("span")
        news_time = time_element.get_text().strip() if time_element else None

        # ‚úÖ Ensure valid timestamp
        news_timestamp = dateparser.parse(news_time) if news_time else datetime.datetime.now()
        if news_timestamp is None:
            news_timestamp = datetime.datetime.now()  # ‚úÖ Prevent NoneType error

        # ‚úÖ Try to extract specific city name
        for city in KERALA_CITIES:
            if city.lower() in title.lower():
                location = city
                break

        # ‚úÖ Define severity levels
        if "death" in title.lower() or "severe" in title.lower():
            severity = "High"
        elif "hospitalized" in title.lower() or "outbreak" in title.lower():
            severity = "Moderate"

        news_data.append(f"{news_timestamp.strftime('%Y-%m-%d %H:%M')}\t{location}\t{disease}\t{severity}")

    return news_data

# ‚úÖ Function to save data to a text file (Overwrite Mode)
def save_to_file(data):
    with open(OUTPUT_FILE, "w", encoding="utf-8") as file:  # "w" mode overwrites old data
        file.write("Timestamp\tLocation\tDisease\tSeverity\n")  # ‚úÖ Header line
        for entry in data:
            file.write(entry + "\n")

# ‚úÖ Main function to run the scraper
def run_scraper():
    print("\nüïí Running Kerala Disease News Scraper (Overwriting old data)...")

    all_news = []
    for disease in DISEASES:
        news = scrape_news(disease)
        all_news.extend(news)

    if all_news:
        save_to_file(all_news)
        print(f"‚úÖ {len(all_news)} records saved to {OUTPUT_FILE} (Overwritten).\n")
    else:
        print("‚ö†Ô∏è No relevant news found.\n")

# ‚úÖ Schedule scraper to run every 2 hours
schedule.every(2).hours.do(run_scraper)

# ‚úÖ Run immediately before scheduling
run_scraper()

print("‚è≥ Waiting for next scheduled run...\n")
while True:
    schedule.run_pending()
    time.sleep(60)  # Checks every minute
